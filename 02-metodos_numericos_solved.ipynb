{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos Numéricos - 2\n",
    "\n",
    "<img src=\"https://github.com/ccdonoso/Intro2_Computational_Mechanics/blob/master/img/logo_usach_dimec.png?raw=true\" alt=\"logo\" width=\"300\"/>\n",
    "\n",
    "- Autor: Claudio Canales Donoso\n",
    "- Página: [ccdonoso.github.io](ccdonoso.github.io)\n",
    "- Cursos: Mecánica Computacional - Diseño Computarizado\n",
    "- Universidad de Santiago de Chile\n",
    "- Fecha: 10/05/21\n",
    "\n",
    "License: BSD 3 clause\n",
    "\n",
    "**Contenido** : Métodos Númericos - 2.\n",
    "- Gradientes Descendientes\n",
    "- Gradientes Conjugados - Sistemas de Ecuaciones Lineales\n",
    "- Cuadratura Gaussiana\n",
    "- Newton Raphson Generalizado \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradientes Descendientes\n",
    "\n",
    "El Descenso de Gradientes [1] es un algoritmo de optimización genérico capaz de encontrar soluciones óptimas a una amplia gama de problemas. La idea general del Descenso de Gradientes es ajustar los parámetros de forma iterativa para minimizar una función de coste $J$.\n",
    "\n",
    "$$ \\min J(\\boldsymbol{\\theta}) $$\n",
    "\n",
    "Supongamos que usted se encuentra perdido en las montañas en medio de una densa niebla, y que sólo puede sentir la pendiente del suelo bajo sus pies. Una buena estrategia para llegar rápidamente al fondo del valle es ir cuesta abajo en la dirección de la pendiente más pronunciada. Esto es exactamente lo que hace el Descenso del Gradiente: mide el gradiente local de la función de error con respecto al vector de parámetros $θ$, y va en la dirección del gradiente descendente. Una vez que el gradiente es cero, ¡se ha alcanzado un mínimo!\n",
    "\n",
    "Concretamente, se empieza llenando $\\boldsymbol{\\theta}$ con valores aleatorios (esto se llama inicialización aleatoria). A continuación, se mejora gradualmente, dando un pequeño paso a la vez, cada paso tratando de disminuir la función de coste (por ejemplo, el MSE), hasta que el algoritmo converge hasta que el algoritmo converja a un mínimo .\n",
    "\n",
    "<img src=\"img/1_laN3aseisIU3T9QTIlob4Q.gif\" alt=\"GD\" width=\"600\"/>\n",
    "\n",
    "<h2 style=\"color:blue\"> El gradiente siempre esta orientado hacia la dirección de máximo crecimiento </h2>\n",
    "\n",
    "\n",
    " \n",
    " \n",
    "**El gradiente de una función siempre esta orientado a la dirección de máximo crecimiento y la dirección opuesta es la dirección de mínimo crecimiento**. En base a esta propiedad del gradiente, se puede utilizar la dirección y magnitud del gradiente para encontrar un máximo o un mínimo. \n",
    "\n",
    "El método consiste en los siguientes pasos:\n",
    "\n",
    "1. Inicialización de un vector de parámetros $\\boldsymbol{\\theta}_0$\n",
    "2. Repetir hasta que un criterio de término se cumpla:\n",
    "    1. $\\boldsymbol{\\theta}_{n+1} = \\boldsymbol{\\theta}_n - \\alpha \\triangledown J(\\boldsymbol{\\theta}_n), \\quad n>0$\n",
    "    \n",
    "Donde $\\alpha$ pertenece a $\\mathbb{R}^{+}$ y tiene que ser lo suficientemente pequeño para que cumpla que $J(\\boldsymbol{\\theta}_{n+1}) < J(\\boldsymbol{\\theta}_n)$\n",
    "\n",
    "En Resumen el método se basa en descender de la cumbre utilizando los gradientes de la función dando pequeños pasos.\n",
    "\n",
    " $$ \\Large \\boldsymbol{\\theta}_{n+1} = \\boldsymbol{\\theta}_n - \\alpha \\triangledown J(\\boldsymbol{\\theta}_n), \\quad n>0 $$\n",
    " \n",
    "Recordar que este método esta condicionado a la inicialización. \n",
    "\n",
    "## Resolver el siguiente problema de optimización\n",
    "\n",
    "$$\\Large \\min f(x) = sin(x) \\: x^2 \\quad \\in [0,2\\pi]$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "import numpy as np\n",
    "\n",
    "# Datos de la Grafica de la función\n",
    "x = np.linspace(0,8) #ndarray\n",
    "f = lambda x: x**2. * np.sin(x)\n",
    "y = f(x)\n",
    "\n",
    "# Setup Graficos\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.axis([0,8,-30,30])\n",
    "l, = ax.plot([],[],'rx')\n",
    "time_text = ax.text(3, 20, '', fontsize=14)\n",
    "ax.plot(x,y)\n",
    "\n",
    "# Ingresa el Código aquí\n",
    "# ---------------------------\n",
    "#Gradiente Descendiente\n",
    "learning_rate = .001\n",
    "grad = lambda x: 2.*x*np.sin(x)+x**2.*np.cos(x)\n",
    "theta_n = [np.pi]\n",
    "iters = 200\n",
    "\n",
    "for i in range(iters):\n",
    "    theta_n.append(theta_n[i] - learning_rate * grad(theta_n[i]))\n",
    "    \n",
    "theta_n = np.array(theta_n)\n",
    "J = f(theta_n)\n",
    "\n",
    "#-------------------------------\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    time_text.set_text(r\"El valor de $\\theta$: {:3f}\".format(theta_n[i])+\" \\nIteración: {}\\nFunción objetivo:{:2f}\".format(i,J[i]))\n",
    "    l.set_data(theta_n[:i], J[:i])\n",
    "    return time_text\n",
    "\n",
    "    \n",
    "ani = matplotlib.animation.FuncAnimation(fig, animate, frames=len(J))\n",
    "\n",
    "\n",
    "ax.legend([\"Iteraciones del método\",\"Función a minimizar\"])\n",
    "\n",
    "plt.close()\n",
    "\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resolver el siguiente problema con gradientes descendientes.\n",
    "\n",
    "Hacer un código, que sea capaz de resolver el siguiente problema de minimización para cualquier $n$ y con inicialización $\\mathbf{0}$.\n",
    "\n",
    "$$\\Large \\min_{\\mathbf{x} \\in \\mathbb{R^n}} f(\\mathbf{x}) = \\frac{1}{2}\\sum_{i=1}^n (x_i-1)^2 $$ \n",
    "\n",
    "Para el caso $n=2$ se tiene que la función es:\n",
    "\n",
    "<img src=\"img/parab2d.png\" alt=\"parabolaa\" width=\"350\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_x2(x,n):\n",
    "    \"\"\"\n",
    "    Esta función calcula la función a minimizar\n",
    "    Parameter\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        El argumento de la función\n",
    "    n : int\n",
    "        Determina la dimensión del problema\n",
    "    Return\n",
    "    ----------\n",
    "    f: float\n",
    "        El valor de la función\n",
    "    \"\"\"\n",
    "    f = 0.\n",
    "    for i in range(n):\n",
    "        f = (x[i]-1.)**2 + f\n",
    "    return f*0.5\n",
    "\n",
    "def grad_sum_x2(x,n):\n",
    "    \"\"\"\n",
    "    Esta función calcular el gradiente\n",
    "    Parameter\n",
    "    ----------\n",
    "    x : ndarray\n",
    "        El argumento de la función\n",
    "    n : int\n",
    "        Determina la dimensión del problema\n",
    "    Return\n",
    "    ----------\n",
    "    grad: ndarray\n",
    "    \"\"\"\n",
    "    grad = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        grad[i] = (x[i]-1.)\n",
    "    return grad\n",
    "\n",
    "n = 40000\n",
    "iterations = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "x = np.zeros(n)\n",
    "\n",
    "for i in range(iterations):\n",
    "    x = x - learning_rate * grad_sum_x2(x,n)\n",
    "    \n",
    "print(x)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solución de Sistemas de ecuaciones Lineales a través de optimización\n",
    "\n",
    "\n",
    "Un sistema de ecuaciones puede ser escrito como\n",
    "\n",
    "$$\\Large \\mathbf{Ax}=\\mathbf{b} $$,\n",
    "\n",
    "donde $\\mathbf{A}$ es una matriz $n\\times n$, las expresiones $\\mathbf{x}$ y $\\mathbf{b}$ son matrices de $n\\times 1$.\n",
    "\n",
    "Este problema puede ser escrito de la siguiente manera:\n",
    "\n",
    "$$ \\Large \\min_{\\mathbf{x} \\in \\mathbb{R^n}}f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x^T}\\mathbf{Ax}-\\mathbf{x^T}\\mathbf{b}  $$\n",
    "\n",
    "La existencia de un minimizador esta garantizada si es que $\\mathbf{A}$ es una matriz simetrica definida positiva.\n",
    "\n",
    "$$\\Large \\triangledown^2 f(\\mathbf{x}) = \\mathbf{A}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método de los Gradientes Conjugados\n",
    "\n",
    "En matemáticas, el método del gradiente conjugado es un algoritmo para la solución numérica de sistemas de ecuaciones lineales determinados, es decir, aquellos cuya matriz sea positiva-definida. El método del gradiente conjugado suele implementarse como un algoritmo iterativo, aplicable a sistemas dispersos que son demasiado grandes para ser tratados por una implementación directa u otros métodos directos como la descomposición de Cholesky. Los sistemas dispersos de gran tamaño suelen surgir al resolver numéricamente ecuaciones diferenciales parciales o problemas de optimización.\n",
    "\n",
    "**El método del gradiente conjugado también puede utilizarse para resolver problemas de optimización sin restricciones, como la minimización de la energía.**\n",
    "\n",
    "El método del **gradiente biconjugado** proporciona una generalización a las matrices no simétricas. Varios métodos de gradiente conjugado no lineal buscan los mínimos de las ecuaciones no lineales y las funciones objetivo de caja negra.\n",
    "\n",
    "**Para poder utilizar este metodo, la matriz tiene que ser definida positiva**\n",
    "\n",
    "Para asegurar que una función presenta un mínimo local en $\\mathbf{x}$, se tiene que satisfacer que \n",
    "\n",
    "$$\\large \\triangledown f(\\mathbf{x}) = \\mathbf{0} ; \\quad \\triangledown^2 f(\\mathbf{x}) \\rightarrow\\textrm{  sea Definido Positivo.}$$\n",
    "\n",
    "Es fácil comprobar que todas las condiciones se cumplen para un sistema de ecuaciones con una matriz simétrica y definida positiva.\n",
    "\n",
    "$$ \\Large \\min_{\\mathbf{x} \\in \\mathbb{R^n}}f(\\mathbf{x})=\\frac{1}{2}\\mathbf{x^T}\\mathbf{Ax}-\\mathbf{x^T}\\mathbf{b}  $$\n",
    "\n",
    "$$ \\Large \\triangledown f(\\mathbf{x})=\\mathbf{Ax-b} = 0 $$\n",
    "\n",
    "Se observa que al encontrar el valor que minimiza la función, al mismo tiempo se resuelve el sistema de ecuaciones lineales y la condición de optimalidad se verifica con la condición del Hessiano.\n",
    "\n",
    "$$\\Large \\triangledown^2 f(\\mathbf{x}) = \\mathbf{A}  $$\n",
    "\n",
    "### Este método utiliza los gradientes descendientes para encontrar el óptimo del problema, pero los pasos que da son conjugados de su paso anterior ( Gradientes Conjugados).\n",
    "\n",
    "Como se puede ver, el gradiente esta definido y el primer vector de la base es $\\mathbf{p_0}$ que toma el valor negativo del gradiente en el primer paso $\\mathbf{p_0} = \\mathbf{b-Ax}$. Los otros vectores de la base serán conjugados de este. Notar que este valor tambien es el residuo del método.\n",
    "\n",
    "$$\\mathbf{r_n}= \\mathbf{b-Ax}$$\n",
    "\n",
    "Como se puede observar de las ecuaciones anteriores, $\\mathbf {r} _{n}$ es el valor negativo del gradiente de $f$ en $ \\mathbf {x} _{k}$, por lo que el método de gradiente descendente requeriría moverse en la dirección $\\mathbf{r}_k$. Aquí, sin embargo, insistimos en que las direcciones $ \\mathbf {p} _{n}$ sean conjugados entre sí. Una forma práctica de aplicar esto es exigir que la siguiente dirección de búsqueda se construya a partir del residuo actual y de todas las direcciones de búsqueda anteriores. La restricción de conjugación es una restricción de tipo ortonormal y, por tanto, el algoritmo puede considerarse un ejemplo de ortonormalización de Gram-Schmidt. Esto da la siguiente expresión:\n",
    "\n",
    "$$\\mathbf {p} _{n}=\\mathbf {r} _{n}-\\sum _{i<n}{\\frac {\\mathbf {p} _{i}^{\\mathsf {T}}\\mathbf {A} \\mathbf {r} _{n}}{\\mathbf {p} _{i}^{\\mathsf {T}}\\mathbf {A} \\mathbf {p} _{i}}}\\mathbf {p} _{i}$$\n",
    "\n",
    "Siguiendo esta dirección, la siguiente ubicación óptima viene dada por:\n",
    "\n",
    "$$ \\mathbf {x} _{n+1}=\\mathbf {x} _{n}+\\alpha _{n}\\mathbf {p} _{n} $$\n",
    "\n",
    "con\n",
    "\n",
    "$$ \\alpha _{n}={\\frac {\\mathbf {p} _{n}^{\\mathsf {T}}(\\mathbf {b} -\\mathbf {Ax} _{n})}{\\mathbf {p} _{n}^{\\mathsf {T}}\\mathbf {A} \\mathbf {p} _{n}}}={\\frac {\\mathbf {p} _{n}^{\\mathsf {T}}\\mathbf {r} _{n}}{\\mathbf {p} _{n}^{\\mathsf {T}}\\mathbf {A} \\mathbf {p} _{n}}}$$\n",
    "\n",
    "## El algoritmo para resolver un sistema de ecuaciones lineales siendo $\\mathbf{A}$ definida positiva es:\n",
    "\n",
    "<img src=\"img/CG.svg\" alt=\"CG-algorithm\" width=\"500\"/>\n",
    "\n",
    "\n",
    "**Es posible observar que este algoritmo es una versión modificada del método de los gradientes descendientes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuadratura Gaussiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el análisis numérico, una regla de cuadratura es una aproximación de la integral definida de una función, normalmente expresada como una suma ponderada de los valores de la función en puntos específicos dentro del dominio de integración. Una regla de cuadratura gaussiana de n puntos, llamada así en honor a Carl Friedrich Gauss, es una regla de cuadratura construida para obtener un resultado exacto para polinomios de grado $2n - 1$ o menos mediante una elección adecuada de los nodos $x_i$ y los pesos $w_i$ para $i = 1,\\dots ,n$. La formulación moderna que utiliza polinomios ortogonales fue desarrollada por Carl Gustav Jacobi en 1826 .El dominio de integración más común para dicha regla se toma como $[-1, 1]$, por lo que la regla se establece como:\n",
    "\n",
    "\n",
    "$$\\int_{-1}^{1}f(x)dx = \\sum_i^n w_i f(x_i) $$\n",
    "\n",
    "Para el problema de integración más simple indicado anteriormente, es decir, $f(x)$ es bien aproximado por polinomios en $[-1,1]$, los polinomios ortogonales asociados son polinomios de Legendre, denotados por $P_n(x)$. Con el n-ésimo polinomio normalizado para dar $P_n(1) = 1$, el i-ésimo nodo de Gauss, $x_i$, es la i-ésima raíz de $P_n$. Estos pesos se pueden calcular como:\n",
    "\n",
    "$$w_i = \\frac{2}{(1-x_i^2)[P'_n(x_i)]^2} $$ \n",
    "\n",
    "<img src=\"img/puntos_pesos_gauss.gif\" alt=\"Cuadratura Gaussina\" width=\"600\"/>\n",
    "\n",
    "A través de  una transformación lineal, es posible cambiar los límites de integración de la siguiente manera:\n",
    "\n",
    "$$\\int_a^{b}f(x)dx =\\frac{b-a}{2} \\sum_i^n w_i f(\\frac{b-a}{2}\\chi_i +\\frac{b+a}{2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcular la siguiente integral utilizando Cuadratura Gaussiana :\n",
    "$$  \\int_{-2}^{1} x^{3} + x^{5} + sin(x) dx = -15.206$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_quadrature3(f,a,b):\n",
    "    \"\"\"\n",
    "    Esta función integra utilizando 3 puntos de cuadratura Gaussiana\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    f : function\n",
    "        función a integrar\n",
    "    a : float\n",
    "        Límite inferior del intervalo de integración\n",
    "    b : float\n",
    "        Límite superior del intervalo de integración\n",
    "    Retorna\n",
    "    ----------\n",
    "    integral : float\n",
    "        Valor de la approximación de la integral\n",
    "    \"\"\"\n",
    "    wi = np.array([5./9.,8./9.,5./9.])\n",
    "    xi = np.array([-np.sqrt(3./5.),0.,np.sqrt(3./5.)])\n",
    "    \n",
    "    integral = 0.\n",
    "    \n",
    "    for i in range(3):\n",
    "        integral = wi[i]*f(xi[i]*(b-a)*0.5+(a+b)*0.5) + integral\n",
    "    \n",
    "    return integral*(b-a)*0.5\n",
    "\n",
    "print(gauss_quadrature3(lambda x: x**3. + x**5. + np.sin(x),-2.,1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:green\"> Desafío: Programar la cuadratura Gaussiana para cualquier $n$. </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_quadrature(f,a,b,n=3):\n",
    "    \"\"\"\n",
    "    Esta función integra utilizando n puntos de cuadratura Gaussiana\n",
    "    \n",
    "    Parámetros\n",
    "    ----------\n",
    "    f : function\n",
    "        función a integrar\n",
    "    a : float\n",
    "        Límite inferior del intervalo de integración\n",
    "    b : float\n",
    "        Límite superior del intervalo de integración\n",
    "    n : int\n",
    "        Cantidad de puntos de la cuadratura Gaussiana\n",
    "    Retorna\n",
    "    ----------\n",
    "    integral : float\n",
    "        Valor de la approximación de la integral\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método de Newton Raphson Generalizado\n",
    "\n",
    "El método de Newton Raphson puede ser utilizado para solucionar sistemas de ecuaciones no-lineales.\n",
    "\n",
    "\n",
    "$$\\left\\{ \\begin{array}{l}\n",
    "f_1(x_1,\\cdots,x_N)=f_1({\\mathbf x})=0\n",
    "\\\\...\n",
    "... \\\\\n",
    "f_N(x_1,\\cdots,x_N)=f_N({\\mathbf x})=0 \\end{array}\\right. ,$$\n",
    "\n",
    "donde\n",
    "\n",
    "$${\\mathbf x}=[x_1,\\cdots,x_N]^T . $$\n",
    "\n",
    "Tambien se puede definir un vector de funciones \n",
    "$${\\mathbf f}({\\mathbf x})=[f_1({\\bf x}),\\cdots,f_n({\\mathbf x})]^T .$$\n",
    "\n",
    "Por lo tanto el sistema de ecuaciones puede ser escrito como:\n",
    "\n",
    "$${\\mathbf f}({\\mathbf x})={\\mathbf0} $$\n",
    "\n",
    "\n",
    "Las series de Taylor nos permiten expresar cada una de las funciones como:\n",
    "\n",
    "$$\\begin{align*}\n",
    "   f(\\mathbf{x}) \\approx f(\\mathbf{a}) + \\triangledown f(\\mathbf{a}) (\\mathbf{x}-\\mathbf{a})\n",
    "   +  \\frac{1}{2} (\\mathbf{x}-\\mathbf{a})^T \\triangledown^2 f(\\mathbf{a}) (\\mathbf{x}-\\mathbf{a}).\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "Es posible utilizar el termino de primer orden para linealizar la ecuación, es decir hasta el gradiente.\n",
    "\n",
    "<img src=\"img/img388.png\" alt=\"NRG_Matriz\" width=\"600\"/>\n",
    "\n",
    "Donde:\n",
    "\n",
    "$$J=\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "\\frac{\\partial}{\\partial x_1}f_1(x_1,x_2,\\cdots,x_n) & \\frac{\\partial}{\\partial x_2}f_1(x_1,x_2,\\cdots,x_n) & \\cdots &\\frac{\\partial}{\\partial x_n}f_1(x_1,x_2,\\cdots,x_n)\\\\\n",
    "\\frac{\\partial}{\\partial x_1}f_2(x_1,x_2,\\cdots,x_n) & \\frac{\\partial}{\\partial x_2}f_2(x_1,x_2,\\cdots,x_n) & \\cdots &\\frac{\\partial}{\\partial x_n}f_2(x_1,x_2,\\cdots,x_n)\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "\\frac{\\partial}{\\partial x_1}f_n(x_1,x_2,\\cdots,x_n) & \\frac{\\partial}{\\partial x_2}f_n(x_1,x_2,\\cdots,x_n) & \\cdots &\\frac{\\partial}{\\partial x_n}f_n(x_1,x_2,\\cdots,x_n)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}$$  \n",
    "\n",
    "\n",
    "Si se dan cuenta, ahora el sistema de ecuación es un lineal y $\\mathbf{f(x)=0}$. Por lo tanto, al linealizar el problema, es posible realizar pequeñas aproximaciones en base a vector de inicialización $\\mathbf{x}_0$ y iterar sucesivamente al igual que el método de Newton-Raphson 1D. Por lo tanto, con un vector  $\\mathbf{x}_0$ se puede obtener un vector $\\mathbf{x}_1=\\mathbf{x}_0 + \\mathbf{\\Delta x_0}$ y ese vector se puede obtener al solucionar el siguiente sistemas de ecuaciones:\n",
    "\n",
    "El algoritmo básico consiste en repetir estos tres pasos:\n",
    "\n",
    "1. Resolver el sistema de ecuaciones:\n",
    "\n",
    "$$ \\Large \\mathbf{J(x_n) \\Delta x_n = - f(x_n) } $$\n",
    "\n",
    "Esta ecuación es semejante a resolver:\n",
    "\n",
    "$$ \\Large \\mathbf{A x = b } $$\n",
    "2. Actualizar el vector solución: $$\\Large \\mathbf{x}_{n+1}=\\mathbf{x}_n + \\Delta \\mathbf{x}_n $$\n",
    "\n",
    "3. Repetir estos pasos hasta que un criterio de término se cumpla:\n",
    "\n",
    "\n",
    "[Revisar este documento que contiene ejemplos de como utilizar el método](https://www.scribd.com/document/57803567/Algoritmo-de-Newton-raphson-Generalizado)\n",
    "\n",
    "Resolver el siguiente problema con $\\mathbf{x_0}=(0,0)$\n",
    "\n",
    "$$5x^2 +3xy-2=0 $$\n",
    "\n",
    "$$x^2+7y^2+3xy-10=0  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacer el código aquí\n",
    "\n",
    "def vector_funciones(x):\n",
    "    vf = np.zeros(2)\n",
    "    vf[0] = 5*x[0]**2+3*x[0]*x[1]-2\n",
    "    vf[1] = x[0]**2+7*x[1]**2+3*x[0]*x[1]-10\n",
    "    return vf\n",
    "\n",
    "\n",
    "def jacob(x): \n",
    "    \"\"\"\n",
    "    df1/dx     df1/dy\n",
    "    df2/dx     df2/dy\n",
    "    \"\"\"\n",
    "    J = np.zeros((2,2))\n",
    "    J[0,0] = 10*x[0] + 3*x[1]\n",
    "    J[0,1] = 3*x[0]\n",
    "    J[1,0] = 2*x[0]+3*x[1]\n",
    "    J[1,1] = 14.*x[1]+3.*x[0]\n",
    "    return J\n",
    "\n",
    "x = np.array([-0.1,-0.1])\n",
    "\n",
    "for i in range(20):\n",
    "    dx = np.linalg.solve(jacob(x), -1.*vector_funciones(x))\n",
    "    x = dx + x         #xn+1\n",
    "\n",
    "residuo = vector_funciones(x)\n",
    "\n",
    "norma = np.dot(residuo,residuo)\n",
    "print(\"El valor del residuo:\",norma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:gray\"> Problema : encontrar la posición de un avión utilizando tres antenas y el método de Newton Raphson </h1>\n",
    "\n",
    "Para monitorear la posición de un avión durante el vuelo, este se comunica de forma simultánea con tres antenas, de forma tal que de cada una de ellas obtiene el tiempo que tarda una señal en ir desde el avión a las antenas. Considerando que las señales viajan a $299.792 km/s$, y utilizando el archivo “tiempo_avion.dat”, encuentre la posición instantánea $x,y,z$ del avión, para cada punto de muestreo, utilizando el método de newton Raphson en varias variables.\n",
    "\n",
    "<img src=\"img/antenas.png.svg\" alt=\"Antenas\" width=\"900\"/>\n",
    "\n",
    "Se le pide guardar la posición $x,y,z$ del avión en un archivo de texto llamado “posicion.dat”\n",
    "\n",
    "$$(x-x_1)^2+(y-y_1)^2+(z-z_1)^2 = D_1^2 $$\n",
    "$$(x-x_2)^2+(y-y_2)^2+(z-z_2)^2 = D_2^2 $$\n",
    "$$(x-x_3)^2+(y-y_3)^2+(z-z_3)^2 = D_3^2 $$\n",
    "\n",
    "Las distancias se pueden calcular como: $d=ct$, donde $c$ es la velocidad de la luz y $t$ es el tiempo de ida de la señal.\n",
    "\n",
    "La posición de las antenas en [km] son:\n",
    "$$A1=(0,50,0)$$\n",
    "$$A2=(60,0,0)$$\n",
    "$$A3=(70,30,0)$$\n",
    "**Consejo, utilice como punto de inicialización $(0,0,0)$ $[km]$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vector_funciones(x,d):\n",
    "    vf = np.zeros(3)\n",
    "    x , y , z = x\n",
    "    d1 , d2, d3 = d\n",
    "    vf[0] = x**2+(y-50.)**2+z**2.-d1**2.\n",
    "    vf[1] = (x-60.)**2 +y**2 + z**2 -d2**2\n",
    "    vf[2] = (x-70.)**2 + (y-30.)**2 + z**2-d3**2.\n",
    "    return vf\n",
    "\n",
    "\n",
    "def jacob(x,d): \n",
    "    \"\"\"\n",
    "    df1/dx     df1/dy   df1/dz\n",
    "    df2/dx     df2/dy   df2/dz\n",
    "    df3/dx     df3/dy   df3/dz\n",
    "    \"\"\"\n",
    "    x , y , z = x\n",
    "    d1 , d2, d3 = d\n",
    "    \n",
    "    J = np.zeros((3,3))\n",
    "\n",
    "    J[0,0] = 2*x\n",
    "    J[0,1] = 2.*(y-50.)\n",
    "    J[0,2] = 2.*z\n",
    "    J[1,0] = 2.*(x-60.)\n",
    "    J[1,1] = 2.*y\n",
    "    J[1,2] = 2.*z\n",
    "    J[2,0] = 2.*(x-70.)\n",
    "    J[2,1] = 2.*(y-30.)\n",
    "    J[2,2] = 2*z\n",
    "    return J\n",
    "\n",
    "\n",
    "tiempos_s = np.loadtxt(fname=\"files/tiempo_avion.dat\")/1000. # columnas están ordenadas como[t1,t2,t3]\n",
    "                                                             # Esta expresión se multiplica por 1000 \n",
    "                                                             # para que el tiempo este en Segundos.\n",
    "vel_luz = 299792. # Vel Luz km/s\n",
    "\n",
    "distancias = tiempos_s*vel_luz   # \"Tiempo_señal x Velocidad\"        \n",
    "\n",
    "mediciones = tiempos_s.shape[0] # int -> Cantidad de filas -> Cantidad de mediciones de los radares\n",
    "\n",
    "x = np.array([0.1,0.1,0.1])\n",
    "sol = np.zeros((mediciones,3))\n",
    "\n",
    "for t in range(mediciones):\n",
    "    for i in range(20):\n",
    "        dx = np.linalg.solve(jacob(x,distancias[t,:]), -1.*vector_funciones(x,distancias[t,:]))\n",
    "        x = dx + x                                                  #xn+1   \n",
    "    sol[t,:] = x                                                    # observar que el x obtenido del paso anterior\n",
    "                                                                    # se utiliza como semilla para t+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución numérica con la información del avión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "xline , yline, zline = sol.T\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "def animate(frame):\n",
    "    ax.view_init(30, frame)\n",
    "    ax.plot3D(xline[:frame], yline[:frame], zline[:frame], color = 'green')\n",
    "    return fig\n",
    "\n",
    "ax.legend([\"Trayectoria de avión\"])\n",
    "plt.close()\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=len(zline))\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solución analítica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import axes3d\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "zline = np.linspace(0, 15, 100)\n",
    "xline = np.sin(zline)\n",
    "yline = np.cos(zline)\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "\n",
    "def animate(frame):\n",
    "    ax.view_init(30, frame)\n",
    "    ax.plot3D(xline[:frame], yline[:frame], zline[:frame], color = 'blue')\n",
    "    return fig\n",
    "\n",
    "ax.legend([\"Trayectoria de avión\"])\n",
    "plt.close()\n",
    "\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, frames=len(zline))\n",
    "HTML(anim.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "[1] Géron, A. (2019). Hands-on machine learning with Scikit-Learn, Keras, and TensorFlow: Concepts, tools, and techniques to build intelligent systems. O'Reilly Media."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
